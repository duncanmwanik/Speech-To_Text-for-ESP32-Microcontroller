{"cells":[{"cell_type":"markdown","metadata":{},"source":["**Last Edited: 26-Feb-2022**  ||  **Authors: Duncan Mwanik**"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Preparation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ynl6bxtxCNHS"},"outputs":[],"source":["# declaring some data locations\n","# DRIVE_STORAGE_PATH = 'C:/Users/Mo/Desktop/Speech/saves2'\n","\n","# the dataset is a folder \"SpeechTest\" containing folders for each word where in are the wav files\n","SPEECH_DATA = 'C:/Users/Mo/Desktop/SpeechTest'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dvunz-vy5lSr"},"outputs":[],"source":["# installing tensorflow\n","%pip install tensorflow.io\n","\n","print('done!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tgwuy-3m5gm9"},"outputs":[],"source":["# importing required packages\n","\n","import tensorflow as tf\n","import numpy as np\n","import tensorflow_io as tfio\n","from tensorflow.python.ops import gen_audio_ops as audio_ops\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from tensorflow.python.ops import gen_audio_ops as audio_ops\n","\n","print('done!')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.io import gfile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wHBgiL355FS"},"outputs":[],"source":["# The audio is all sampled at 16KHz and should all be 1 second in length - so 1 second is 16000 samples\n","EXPECTED_SAMPLES=16000\n","# Noise floor to detect if any audio is present\n","NOISE_FLOOR=0.1\n","# How many samples should be abover the noise floor?\n","MINIMUM_VOICE_LENGTH=EXPECTED_SAMPLES/4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ET-Zlc86RMk"},"outputs":[],"source":["# list of folders we want to process in the speech_data folder\n","command_words = [\n","    'on',\n","    'off',\n","    'zero',\n","    'one',\n","    'two',\n","    'three',\n","    'four',\n","    'five',\n","    'six',\n","    'seven',\n","    'eight',\n","    'nine',\n","    '_invalid',\n","]\n","nonsense_words = [\n","    'stop',\n","    'go',\n","    'forward',\n","    'backward',\n","    'left',\n","    'right',\n","    'up',\n","    'down',\n","    'learn',\n","    'yes',\n","    'no',\n","    'follow',\n","    'tree',\n","    'bed',\n","    'bird',\n","    'cat',\n","    'dog',\n","    'happy',\n","    'house',\n","    'marvin',\n","    'sheila',\n","    'visual',\n","    'wow',\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sRltfVz6RxD"},"outputs":[],"source":["# get all the files in a directory\n","def get_files(word):\n","    return gfile.glob(SPEECH_DATA + '/'+ word +'/*.wav')\n","\n","# get the location of the voice\n","def get_voice_position(audio, noise_floor):\n","    audio = audio - np.mean(audio)\n","    audio = audio / np.max(np.abs(audio))\n","    return tfio.experimental.audio.trim(audio, axis=0, epsilon=noise_floor)\n","\n","# Work out how much of the audio file is actually voice\n","def get_voice_length(audio, noise_floor):\n","    position = get_voice_position(audio, noise_floor)\n","    return (position[1] - position[0]).numpy()\n","\n","# is enough voice present?\n","def is_voice_present(audio, noise_floor, required_length):\n","    voice_length = get_voice_length(audio, noise_floor)\n","    return voice_length >= required_length\n","\n","# is the audio the correct length?\n","def is_correct_length(audio, expected_length):\n","    return (audio.shape[0]==expected_length).numpy()\n","\n","def is_valid_file(file_name):\n","    # load the audio file\n","    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n","    # check the file is long enough\n","    if not is_correct_length(audio_tensor, EXPECTED_SAMPLES):\n","        return False\n","    # convert the audio to an array of floats and scale it to betweem -1 and 1\n","    audio = tf.cast(audio_tensor[:], tf.float32)\n","    audio = audio - np.mean(audio)\n","    audio = audio / np.max(np.abs(audio))\n","    # is there any voice in the audio?\n","    if not is_voice_present(audio, NOISE_FLOOR, MINIMUM_VOICE_LENGTH):\n","        return False\n","    return True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rn3_VZk_6SJD"},"outputs":[],"source":["# A function to get a spectogram from each audio file\n","\n","def get_spectrogram(audio):\n","    # normalise the audio\n","    audio = audio - np.mean(audio)\n","    audio = audio / np.max(np.abs(audio))\n","    # create the spectrogram\n","    spectrogram = audio_ops.audio_spectrogram(audio,\n","                                              window_size=320,\n","                                              stride=160,\n","                                              magnitude_squared=True).numpy()\n","    # reduce the number of frequency bins in our spectrogram to a more sensible level\n","    spectrogram = tf.nn.pool(\n","        input=tf.expand_dims(spectrogram, -1),\n","        window_shape=[1, 6],\n","        strides=[1, 6],\n","        pooling_type='AVG',\n","        padding='SAME')\n","    spectrogram = tf.squeeze(spectrogram, axis=0)\n","    spectrogram = np.log10(spectrogram + 1e-6)\n","    return spectrogram"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfpL2cOd8FZC"},"outputs":[],"source":["# process a file into its spectrogram\n","def process_file(file_path):\n","    # load the audio file\n","    audio_tensor = tfio.audio.AudioIOTensor(file_path)\n","    # convert the audio to an array of floats and scale it to betweem -1 and 1\n","    audio = tf.cast(audio_tensor[:], tf.float32)\n","    audio = audio - np.mean(audio)\n","    audio = audio / np.max(np.abs(audio))\n","    # randomly reposition the audio in the sample\n","    voice_start, voice_end = get_voice_position(audio, NOISE_FLOOR)\n","    end_gap=len(audio) - voice_end\n","    random_offset = np.random.uniform(0, voice_start+end_gap)\n","    audio = np.roll(audio,-random_offset+end_gap)\n","    # add some random background noise\n","    background_volume = np.random.uniform(0, 0.1)\n","    # get the background noise files\n","    background_files = get_files('_background_noise_')\n","    background_file = np.random.choice(background_files)\n","    background_tensor = tfio.audio.AudioIOTensor(background_file)\n","    background_start = np.random.randint(0, len(background_tensor) - 16000)\n","    # normalise the background noise\n","    background = tf.cast(background_tensor[background_start:background_start+16000], tf.float32)\n","    background = background - np.mean(background)\n","    background = background / np.max(np.abs(background))\n","    # mix the audio with the scaled background\n","    audio = audio + background_volume * background\n","    # get the spectrogram\n","    return get_spectrogram(audio)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nd6tI2PL8GHy"},"outputs":[],"source":["train = []\n","validate = []\n","test = []\n","\n","TRAIN_SIZE=0.8\n","VALIDATION_SIZE=0.1\n","TEST_SIZE=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DG9a5waQ8Gdi"},"outputs":[],"source":["def process_files(file_names, label, repeat=1):\n","    file_names = tf.repeat(file_names, repeat).numpy()\n","    return [(process_file(file_name), label) for file_name in tqdm(file_names, desc=f\"{word} ({label})\", leave=False)]\n","\n","# process the files for a word into the spectrogram and one hot encoding word value\n","def process_word(word, label, repeat=1):\n","    # get a list of files names for the word\n","    file_names = [file_name for file_name in tqdm(get_files(word), desc=\"Checking\", leave=False) if is_valid_file(file_name)]\n","    # randomly shuffle the filenames\n","    np.random.shuffle(file_names)\n","    # split the files into train, validate and test buckets\n","    train_size=int(TRAIN_SIZE*len(file_names))\n","    validation_size=int(VALIDATION_SIZE*len(file_names))\n","    test_size=int(TEST_SIZE*len(file_names))\n","    # get the training samples\n","    train.extend(\n","        process_files(\n","            file_names[:train_size],\n","            label,\n","            repeat=repeat\n","        )\n","    )\n","    # and the validation samples\n","    validate.extend(\n","        process_files(\n","            file_names[train_size:train_size+validation_size],\n","            label,\n","            repeat=repeat\n","        )\n","    )\n","    # and the test samplesw\n","    test.extend(\n","        process_files(\n","            file_names[train_size+validation_size:],\n","            label,\n","            repeat=repeat\n","        )\n","    )\n","\n","# process all the command words\n","for word in tqdm(command_words, desc=\"Processing words\"):\n","    if '_' not in word:\n","        process_word(word, command_words.index(word), repeat=40)\n","        \n","# all the nonsense words\n","for word in tqdm(nonsense_words, desc=\"Processing words\"):\n","    if '_' not in word:\n","        process_word(word, command_words.index('_invalid'), repeat=1)\n","\n","print(len(train), len(test), len(validate))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-51AMgmL8Gu5"},"outputs":[],"source":["# process the background noise files\n","def process_background(file_name, label):\n","    # load the audio file\n","    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n","    audio = tf.cast(audio_tensor[:], tf.float32)\n","    audio_length = len(audio)\n","    samples = []\n","    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 16000), desc=file_name, leave=False):\n","        section_end = section_start + EXPECTED_SAMPLES\n","        section = audio[section_start:section_end]\n","        # get the spectrogram\n","        spectrogram = get_spectrogram(section)\n","        samples.append((spectrogram, label))\n","\n","    # simulate random utterances\n","    for section_index in tqdm(range(1000), desc=\"Simulated Words\", leave=False):\n","        section_start = np.random.randint(0, audio_length - EXPECTED_SAMPLES)\n","        section_end = section_start + EXPECTED_SAMPLES\n","        section = np.reshape(audio[section_start:section_end], (EXPECTED_SAMPLES))\n","\n","        result = np.zeros((EXPECTED_SAMPLES))\n","        # create a pseudo bit of voice\n","        voice_length = np.random.randint(MINIMUM_VOICE_LENGTH/2, EXPECTED_SAMPLES)\n","        voice_start = np.random.randint(0, EXPECTED_SAMPLES - voice_length)\n","        hamming = np.hamming(voice_length)\n","        # amplify the voice section\n","        result[voice_start:voice_start+voice_length] = hamming * section[voice_start:voice_start+voice_length]\n","        # get the spectrogram\n","        spectrogram = get_spectrogram(np.reshape(section, (16000, 1)))\n","        samples.append((spectrogram, label))\n","        \n","    \n","    np.random.shuffle(samples)\n","    \n","    train_size=int(TRAIN_SIZE*len(samples))\n","    validation_size=int(VALIDATION_SIZE*len(samples))\n","    test_size=int(TEST_SIZE*len(samples))\n","    \n","    train.extend(samples[:train_size])\n","\n","    validate.extend(samples[train_size:train_size+validation_size])\n","\n","    test.extend(samples[train_size+validation_size:])\n","\n","        \n","for file_name in tqdm(get_files('_background_noise_'), desc=\"Processing Background Noise\"):\n","    process_background(file_name, command_words.index(\"_invalid\"))\n","    \n","print(len(train), len(test), len(validate))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["dbcb13b28b474d6a8a385e5a4eb5be2f","4e7afd2edfe24566baa9e5234b30b006","f5fb5343f61349a283b254212d516b79","05d382133b4a4e998653bcb7c7a89152","063b60b102bf45eab9c0413c75785d99","27cd77bbb5c5469db6c08f2af5dbd59e","6ce91551f5014b7f9bdd57d74802f46c","10b53a623ba2414e997c432c07dba101"]},"executionInfo":{"elapsed":1501,"status":"ok","timestamp":1614689936237,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"Gy2l-I6n8HFR","outputId":"b864f468-6c8a-48cf-e4b3-887c532f8f6b"},"outputs":[],"source":["def process_problem_noise(file_name, label):\n","    samples = []\n","    # load the audio file\n","    audio_tensor = tfio.audio.AudioIOTensor(file_name)\n","    audio = tf.cast(audio_tensor[:], tf.float32)\n","    audio_length = len(audio)\n","    samples = []\n","    for section_start in tqdm(range(0, audio_length-EXPECTED_SAMPLES, 800), desc=file_name, leave=False):\n","        section_end = section_start + EXPECTED_SAMPLES\n","        section = audio[section_start:section_end]\n","        # get the spectrogram\n","        spectrogram = get_spectrogram(section)\n","        samples.append((spectrogram, label))\n","        \n","    np.random.shuffle(samples)\n","    \n","    train_size=int(TRAIN_SIZE*len(samples))\n","    validation_size=int(VALIDATION_SIZE*len(samples))\n","    test_size=int(TEST_SIZE*len(samples))\n","    \n","    train.extend(samples[:train_size])\n","    validate.extend(samples[train_size:train_size+validation_size])\n","    test.extend(samples[train_size+validation_size:])\n","\n","\n","for file_name in tqdm(get_files(\"_problem_noise_\"), desc=\"Processing problem noise\"):\n","    process_problem_noise(file_name, command_words.index(\"_invalid\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1653,"status":"ok","timestamp":1614689940832,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"6-D-ZCDs8Hag","outputId":"be9ae141-19a0-4f1e-ae22-fe9893041882"},"outputs":[],"source":["print(len(train), len(test), len(validate))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rivqB9a8fRk"},"outputs":[],"source":["# randomise the training samples\n","np.random.shuffle(train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puWtplWg8fny"},"outputs":[],"source":["X_train, Y_train = zip(*train)\n","X_validate, Y_validate = zip(*validate)\n","X_test, Y_test = zip(*test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10165,"status":"ok","timestamp":1614689955706,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"QscqsnRX8f9X","outputId":"dd05799b-f463-48af-d70b-f2bbdb165684"},"outputs":[],"source":["# save the computed data\n","np.savez_compressed(\n","    \"training_spectrogram.npz\",\n","    X=X_train, Y=Y_train)\n","print(\"Saved training data\")\n","np.savez_compressed(\n","    \"validation_spectrogram.npz\",\n","    X=X_validate, Y=Y_validate)\n","print(\"Saved validation data\")\n","np.savez_compressed(\n","    \"test_spectrogram.npz\",\n","    X=X_test, Y=Y_test)\n","print(\"Saved test data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKBggCLB8gMi"},"outputs":[],"source":["# get the width and height of the spectrogram \"image\"\n","IMG_WIDTH=X_train[0].shape[0]\n","IMG_HEIGHT=X_train[0].shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpzFgsEP8gb6"},"outputs":[],"source":["def plot_images2(images_arr, imageWidth, imageHeight):\n","    fig, axes = plt.subplots(2, 5, figsize=(10, 10))\n","    axes = axes.flatten()\n","    for img, ax in zip(images_arr, axes):\n","        ax.imshow(np.reshape(img, (imageWidth, imageHeight)))\n","        ax.axis(\"off\")\n","    plt.tight_layout()\n","    plt.show()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":674},"executionInfo":{"elapsed":2918,"status":"ok","timestamp":1614689970400,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"LGC7p-Pm8gpa","outputId":"e8834dbe-5e23-4582-d29d-29810f4ac2e3"},"outputs":[],"source":["word_index = command_words.index(\"one\")\n","\n","X_left = np.array(X_train)[np.array(Y_train) == word_index]\n","plot_images2(X_left[:10], IMG_WIDTH, IMG_HEIGHT)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":674},"executionInfo":{"elapsed":3006,"status":"ok","timestamp":1614689977303,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"juZjw4_08ys0","outputId":"24533d8d-c1d6-48a7-b429-5ab1739fe4db"},"outputs":[],"source":["word_index = command_words.index(\"nine\")\n","\n","X_right = np.array(X_train)[np.array(Y_train) == word_index]\n","plot_images2(X_right[:10], IMG_WIDTH, IMG_HEIGHT)"]},{"cell_type":"markdown","metadata":{"id":"7ZuigSoBOgNw"},"source":["# **Training**\n","\n","This treats the spectrograms of the words like images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aa6niXGfOgNx"},"outputs":[],"source":["# Import all the things we will need\n","import datetime\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKSFOZvEOgN3"},"outputs":[],"source":["# Load the TensorBoard notebook extension - if you want it inline - this can be a bit flaky...\n","# %load_ext tensorboard\n","# %reload_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48WrGf1GAoZ6"},"outputs":[],"source":["# clear out any old logs\n","# !rm -rf ./logs/ "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XW7YzGxSOgOT"},"outputs":[],"source":["# launch tensorboard using this command\n","# %tensorboard --logdir logs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1292,"status":"ok","timestamp":1614689997871,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"wZ2-CxxZAoZ9","outputId":"71daafb1-f925-493f-8daa-768e2ed75c29"},"outputs":[],"source":["print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EENm2XtOgN6"},"outputs":[],"source":["# List of the words in categorical order\n","command_words = [\n","    'on',\n","    'off',\n","    'zero',\n","    'one',\n","    'two',\n","    'three',\n","    'four',\n","    'five',\n","    'six',\n","    'seven',\n","    'eight',\n","    'nine',\n","    '_invalid',\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERCnsdwMOgN9"},"outputs":[],"source":["# Load up the sprectrograms and labels\n","training_spectrogram = np.load('training_spectrogram.npz')\n","validation_spectrogram = np.load('validation_spectrogram.npz')\n","test_spectrogram = np.load('test_spectrogram.npz')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4L_A47mDOgN_"},"outputs":[],"source":["# extract the data from the files\n","X_train = training_spectrogram['X']\n","Y_train_cats = training_spectrogram['Y']\n","X_validate = validation_spectrogram['X']\n","Y_validate_cats = validation_spectrogram['Y']\n","X_test = test_spectrogram['X']\n","Y_test_cats = test_spectrogram['Y']\n","\n","# get the width and height of the spectrogram \"image\"\n","IMG_WIDTH=X_train[0].shape[0]\n","IMG_HEIGHT=X_train[0].shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"executionInfo":{"elapsed":1866,"status":"ok","timestamp":1614690008780,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"IquLGwB6pSwR","outputId":"f45f35ab-ca18-4b15-dea4-a25be1aa9484"},"outputs":[],"source":["# plot a distribution of the words\n","plt.hist(Y_train_cats, bins=range(0,len(command_words)+1), align='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1215,"status":"ok","timestamp":1614690028738,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"ZXHaeU4uqLnM","outputId":"c9531041-44fa-46b3-beaf-72a7ae955ea0"},"outputs":[],"source":["unique, counts = np.unique(Y_train_cats, return_counts=True)\n","print(unique, counts)\n","dict(zip([command_words[i] for i in unique], counts))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"946JWm6Fe2Wd"},"outputs":[],"source":["Y_train = tf.one_hot(Y_train_cats, len(command_words))\n","Y_validate = tf.one_hot(Y_validate_cats, len(command_words))\n","Y_test = tf.one_hot(Y_test_cats, len(command_words))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-60v8-m3OgOF"},"outputs":[],"source":["# create the datasets for training\n","batch_size = 32\n","\n","train_dataset = Dataset.from_tensor_slices(\n","    (X_train, Y_train)\n",").repeat(\n","    count=-1\n",").shuffle(\n","    len(X_train)\n",").batch(\n","    batch_size\n",")\n","\n","validation_dataset = Dataset.from_tensor_slices((X_validate, Y_validate)).batch(X_validate.shape[0]//10)\n","\n","test_dataset = Dataset.from_tensor_slices((X_test, Y_test)).batch(len(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":907,"status":"ok","timestamp":1614690052345,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"Wbnijb64OgOM","outputId":"e0d8e5c6-0e57-47b8-d9fa-63998f2bb7aa"},"outputs":[],"source":["model = Sequential([\n","    Conv2D(4, 3, \n","           padding='same',\n","           activation='relu',\n","           kernel_regularizer=regularizers.l2(0.001),\n","           name='conv_layer1',\n","           input_shape=(IMG_WIDTH, IMG_HEIGHT, 1)),\n","    MaxPooling2D(name='max_pooling1', pool_size=(2,2)),\n","    Conv2D(4, 3, \n","           padding='same',\n","           activation='relu',\n","           kernel_regularizer=regularizers.l2(0.001),\n","           name='conv_layer2'),\n","    MaxPooling2D(name='max_pooling3', pool_size=(2,2)),\n","    Flatten(),\n","    Dropout(0.1),\n","    Dense(\n","        80,\n","        activation='relu',\n","        kernel_regularizer=regularizers.l2(0.001),\n","        name='hidden_layer1'\n","    ),\n","    Dropout(0.1),\n","    Dense(\n","        len(command_words), \n","        activation='softmax',\n","        kernel_regularizer=regularizers.l2(0.001),\n","        name='output'\n","    )\n","])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnEC2AOdOgOO"},"outputs":[],"source":["epochs=10\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.CategoricalCrossentropy(),\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"pUzYHZJ2OgOR"},"source":["# **Logging to tensorboard**\n","We log the training stats along with the confusion matrix of the test data - should we be using the validation data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwBFfW6TOgOR"},"outputs":[],"source":["log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"]},{"cell_type":"markdown","metadata":{"id":"me2W-gteOgOT"},"source":["# **Train model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141774,"status":"ok","timestamp":1614690220113,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"9s--XYYIOgOW","outputId":"0f773866-44a7-4c33-bc19-67b56ee6d1e5"},"outputs":[],"source":["model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=\"checkpoint.model\",\n","    monitor='val_accuracy',\n","    mode='max',\n","    save_best_only=True)\n","\n","history = model.fit(\n","    train_dataset,\n","    steps_per_epoch=len(X_train) // batch_size,\n","    epochs=epochs,\n","    validation_data=validation_dataset,\n","    validation_steps=10,\n","    callbacks=[tensorboard_callback, model_checkpoint_callback]\n",")\n","\n","print(\"done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2830,"status":"ok","timestamp":1614690225138,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"v1u5dvHuYpsg","outputId":"3a70ec57-de02-4570-d5f5-55a9fee33279"},"outputs":[],"source":["model.save(\"trained_test0.model\")"]},{"cell_type":"markdown","metadata":{"id":"s8Vok4uqFv6_"},"source":["# **Testing the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21m-pPzgFmhD"},"outputs":[],"source":["model2 =  keras.models.load_model(\"trained_test0.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1819,"status":"ok","timestamp":1614690238696,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"74brwGyVGZ7B","outputId":"770afa2f-0ade-42f3-fcb8-371c32c12849"},"outputs":[],"source":["results = model2.evaluate(X_test, tf.cast(Y_test, tf.float32), batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRkkvWOKM6hv"},"outputs":[],"source":["predictions = model2.predict(X_test, 128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0atHifjyAoaQ"},"outputs":[],"source":["import itertools\n","\n","\n","def plot_confusion_matrix(cm, class_names):\n","    \"\"\"\n","  Returns a matplotlib figure containing the plotted confusion matrix.\n","\n","  Args:\n","    cm (array, shape = [n, n]): a confusion matrix of integer classes\n","    class_names (array, shape = [n]): String names of the integer classes\n","  \"\"\"\n","    cm = cm.numpy()\n","    # Normalize the confusion matrix.\n","    cm = np.around(cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n","\n","    figure = plt.figure(figsize=(8, 8))\n","    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n","    plt.title(\"Confusion matrix\")\n","    plt.colorbar()\n","    tick_marks = np.arange(len(class_names))\n","    plt.xticks(tick_marks, class_names, rotation=45)\n","    plt.yticks(tick_marks, class_names)\n","\n","    # Use white text if squares are dark; otherwise black.\n","    threshold = cm.max() / 2.0\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        color = \"white\" if cm[i, j] > threshold else \"black\"\n","        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n","\n","    plt.tight_layout()\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")\n","    plt.show()\n","#     return figure"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":2100,"status":"ok","timestamp":1614690249650,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"YAztSLcIAoaS","outputId":"1bb9ced6-a275-4232-e311-4812af8eada9","scrolled":false},"outputs":[],"source":["cm = tf.math.confusion_matrix(\n","    labels=tf.argmax(Y_test, 1), predictions=tf.argmax(predictions, 1)\n",")\n","\n","plot_confusion_matrix(cm, command_words)"]},{"cell_type":"markdown","metadata":{"id":"TZBWyVSTAoaU"},"source":["# **Fully train the model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZItwJx7yWpY"},"outputs":[],"source":["batch_size = 30\n","complete_train_X = np.concatenate((X_train, X_validate, X_test))\n","complete_train_Y = np.concatenate((Y_train, Y_validate, Y_test))\n","\n","complete_train_dataset = Dataset.from_tensor_slices((complete_train_X, complete_train_Y)).repeat(count=-1).shuffle(len(complete_train_X)).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83045,"status":"ok","timestamp":1614690371748,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"R6YYR5ixAoaX","outputId":"71ecf7e6-fad2-4dd2-8113-451d74be78f9"},"outputs":[],"source":["history = model2.fit(\n","    complete_train_dataset,\n","    steps_per_epoch=len(complete_train_X) // batch_size,\n","    epochs=5\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2930,"status":"ok","timestamp":1614692235047,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"tt5EOwjQAoaY","outputId":"4e701288-970e-4220-aea2-1fc331aabe0c"},"outputs":[],"source":["model2.save(\"fully_trained_test0.model\")"]},{"cell_type":"markdown","metadata":{"id":"7gzEu0HvvnpY"},"source":["Getting Some Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7386,"status":"ok","timestamp":1614692271521,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"4ReLgF_oAoaZ","outputId":"528e55ef-3668-4436-c113-7b46a3b1d922"},"outputs":[],"source":["results = model2.evaluate(complete_train_X, tf.cast(complete_train_Y, tf.float32), batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xz5VNNc0Aoaa"},"outputs":[],"source":["predictions = model2.predict(complete_train_X, 128)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":1573,"status":"ok","timestamp":1614692285957,"user":{"displayName":"JUST Mo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKwqjwd4qcyY3QeyCy0tru4zNN9jiCjGe_v7jf=s64","userId":"13577232840169157848"},"user_tz":540},"id":"vYNTazrkAoab","outputId":"5d2cff0d-7494-4da2-8aaa-4c86d9fecded"},"outputs":[],"source":["cm = tf.math.confusion_matrix(\n","    labels=tf.argmax(complete_train_Y, 1), predictions=tf.argmax(predictions, 1)\n",")\n","\n","plot_confusion_matrix(cm, command_words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf56L80CxQnJ"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7y85DW_rvY6Y"},"source":["# **Converting a trained model to tflite**\n","https://www.tensorflow.org/lite/microcontrollers/build_convert#model_conversion"]},{"cell_type":"markdown","metadata":{"id":"uG9i10dBvY6l"},"source":["# **Convert model to tflite**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2PhrLFCvY6m"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0y6fsxntvY6n"},"outputs":[],"source":["training_spectrogram = np.load('training_spectrogram.npz')\n","validation_spectrogram = np.load('validation_spectrogram.npz')\n","test_spectrogram = np.load('test_spectrogram.npz')\n","\n","X_train = training_spectrogram['X']\n","X_validate = validation_spectrogram['X']\n","X_test = test_spectrogram['X']\n","\n","complete_train_X = np.concatenate((X_train, X_validate, X_test))\n","# complete_train_X = X_validate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_IuSeXsvY6o"},"outputs":[],"source":["converter2 = tf.lite.TFLiteConverter.from_saved_model(\"fully_trained_test0.model\")\n","converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n","def representative_dataset_gen():\n","    for i in range(0, len(complete_train_X), 100):\n","        # Get sample input data as a numpy array in a method of your choosing.\n","        yield [complete_train_X[i:i+100]]\n","converter2.representative_dataset = representative_dataset_gen\n","# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n","converter2.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","tflite_quant_model = converter2.convert()\n","open(\"C:/Users/Mo/Desktop/Speech/converted_model.tflite\", \"wb\").write(tflite_quant_model)"]},{"cell_type":"markdown","metadata":{"id":"LBkFZdpxvY6p"},"source":["# **To convert to C++**\n","This will run a command line too to convert out tflite model into C code. Linux only!!!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"roW6tDeDvY6p"},"outputs":[],"source":["!xxd -i converted_model.tflite > model_data.cc"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"HomieTrain1.ipynb","provenance":[{"file_id":"1MIpkENYwLz7mGgAreqW_JIx3U0uV7rdA","timestamp":1645352442606},{"file_id":"1MFoeB0KfCQti6fM50Wv38zmr48Ti1pPI","timestamp":1611672242405}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"05d382133b4a4e998653bcb7c7a89152":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10b53a623ba2414e997c432c07dba101","placeholder":"â€‹","style":"IPY_MODEL_6ce91551f5014b7f9bdd57d74802f46c","value":" 0/0 [00:03&lt;?, ?it/s]"}},"063b60b102bf45eab9c0413c75785d99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"10b53a623ba2414e997c432c07dba101":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27cd77bbb5c5469db6c08f2af5dbd59e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e7afd2edfe24566baa9e5234b30b006":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ce91551f5014b7f9bdd57d74802f46c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbcb13b28b474d6a8a385e5a4eb5be2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5fb5343f61349a283b254212d516b79","IPY_MODEL_05d382133b4a4e998653bcb7c7a89152"],"layout":"IPY_MODEL_4e7afd2edfe24566baa9e5234b30b006"}},"f5fb5343f61349a283b254212d516b79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Processing problem noise: ","description_tooltip":null,"layout":"IPY_MODEL_27cd77bbb5c5469db6c08f2af5dbd59e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_063b60b102bf45eab9c0413c75785d99","value":0}}}}},"nbformat":4,"nbformat_minor":0}
